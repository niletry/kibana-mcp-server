#!/usr/bin/env python3
"""
Kibana MCP Server
æä¾› Kibana æ—¥å¿—æŸ¥è¯¢åŠŸèƒ½ï¼Œè‡ªåŠ¨å¤„ç†è®¤è¯å’Œä¼šè¯ç®¡ç†
"""

import asyncio
import json
import os
import sys
from datetime import datetime, timedelta
from typing import Any, Optional
from urllib.parse import quote

import httpx
from mcp.server import Server
from mcp.types import (
    Resource,
    Tool,
    TextContent,
    ImageContent,
    EmbeddedResource,
)
from pydantic import BaseModel, Field


# é…ç½®
KIBANA_URL = os.getenv("KIBANA_URL", "https://logs.example.com")
KIBANA_VERSION = os.getenv("KIBANA_VERSION", "8.17.1")
SESSION_TIMEOUT = timedelta(hours=23)  # sid cookie é€šå¸¸ 24 å°æ—¶è¿‡æœŸ


class KibanaSession:
    """ç®¡ç† Kibana ä¼šè¯å’Œè®¤è¯"""
    
    def __init__(self, username: str, password: str):
        self.username = username
        self.password = password
        self.sid_cookie: Optional[str] = None
        self.session_created_at: Optional[datetime] = None
        self.client = httpx.AsyncClient(timeout=30.0)
    
    async def ensure_authenticated(self) -> None:
        """ç¡®ä¿å½“å‰ä¼šè¯å·²è®¤è¯ï¼Œå¦‚æœæœªè®¤è¯æˆ–è¿‡æœŸåˆ™é‡æ–°ç™»å½•"""
        if self._is_session_valid():
            return
        
        await self.login()
    
    def _is_session_valid(self) -> bool:
        """æ£€æŸ¥ä¼šè¯æ˜¯å¦æœ‰æ•ˆ"""
        if not self.sid_cookie or not self.session_created_at:
            return False
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if datetime.now() - self.session_created_at > SESSION_TIMEOUT:
            return False
        
        return True
    
    async def login(self) -> None:
        """ç™»å½• Kibana è·å– sid cookie"""
        login_url = f"{KIBANA_URL}/internal/security/login"
        
        payload = {
            "providerType": "basic",
            "providerName": "cloud-basic",
            "currentURL": f"{KIBANA_URL}/login?msg=LOGGED_OUT",
            "params": {
                "username": self.username,
                "password": self.password
            }
        }
        
        headers = {
            "Content-Type": "application/json",
            "kbn-version": KIBANA_VERSION,
            "kbn-xsrf": "kibana"
        }
        
        try:
            print(f"DEBUG: Attempting login to {login_url} with user {self.username}", file=sys.stderr)
            response = await self.client.post(
                login_url,
                json=payload,
                headers=headers
            )
            
            if response.status_code == 200:
                # æå– sid cookie
                set_cookie = response.headers.get("set-cookie", "")
                if "sid=" in set_cookie:
                    # æå– sid å€¼
                    sid_start = set_cookie.find("sid=") + 4
                    sid_end = set_cookie.find(";", sid_start)
                    self.sid_cookie = set_cookie[sid_start:sid_end]
                    self.session_created_at = datetime.now()
                else:
                    raise Exception("ç™»å½•æˆåŠŸä½†æœªè·å–åˆ° sid cookie")
            else:
                raise Exception(f"ç™»å½•å¤±è´¥: {response.status_code} - {response.text}")
        
        except Exception as e:
            raise Exception(f"Kibana ç™»å½•é”™è¯¯: {str(e)}")
    
    async def request(self, path: str, query: dict) -> dict:
        """
        å‘é€è¯·æ±‚åˆ° Kibana proxy ç«¯ç‚¹
        è‡ªåŠ¨å¤„ç† 401 é”™è¯¯å¹¶é‡æ–°ç™»å½•
        """
        await self.ensure_authenticated()
        
        encoded_path = quote(path, safe='')
        url = f"{KIBANA_URL}/api/console/proxy?path={encoded_path}&method=POST"
        
        headers = {
            "Cookie": f"sid={self.sid_cookie}",
            "kbn-xsrf": "kibana",
            "kbn-version": KIBANA_VERSION,
            "Content-Type": "application/json",
            "x-elastic-internal-origin": "Kibana"
        }
        
        try:
            response = await self.client.post(
                url,
                json=query,
                headers=headers
            )
            
            # å¦‚æœé‡åˆ° 401ï¼Œé‡æ–°ç™»å½•åé‡è¯•
            if response.status_code == 401:
                await self.login()
                
                # æ›´æ–° headers ä¸­çš„ cookie
                headers["Cookie"] = f"sid={self.sid_cookie}"
                
                # é‡è¯•è¯·æ±‚
                response = await self.client.post(
                    url,
                    json=query,
                    headers=headers
                )
            
            if response.status_code == 200:
                return response.json()
            else:
                raise Exception(f"è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
        
        except Exception as e:
            raise Exception(f"Kibana è¯·æ±‚é”™è¯¯: {str(e)}")
    
    async def close(self):
        """å…³é—­ HTTP å®¢æˆ·ç«¯"""
        await self.client.aclose()


# å…¨å±€ä¼šè¯ç®¡ç†å™¨
_session: Optional[KibanaSession] = None


def get_session() -> KibanaSession:
    """è·å–å½“å‰ä¼šè¯"""
    if _session is None:
        raise Exception("Kibana ä¼šè¯æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè®¾ç½®ç”¨æˆ·åå’Œå¯†ç ")
    return _session


def set_credentials(username: str, password: str) -> None:
    """è®¾ç½® Kibana ç™»å½•å‡­è¯"""
    global _session
    if _session:
        asyncio.create_task(_session.close())
    _session = KibanaSession(username, password)


# åˆ›å»º MCP Server
app = Server("kibana")


@app.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„å·¥å…·"""
    return [
        Tool(
            name="kibana_set_credentials",
            description="è®¾ç½® Kibana ç™»å½•å‡­è¯ï¼ˆç”¨æˆ·åå’Œå¯†ç ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "username": {
                        "type": "string",
                        "description": "Kibana ç”¨æˆ·å"
                    },
                    "password": {
                        "type": "string",
                        "description": "Kibana å¯†ç "
                    }
                },
                "required": ["username", "password"]
            }
        ),
        Tool(
            name="kibana_search_logs",
            description="æœç´¢ Kibana æ—¥å¿—ã€‚æ”¯æŒè‡ªå®šä¹‰ Elasticsearch DSL æŸ¥è¯¢æˆ–ç®€å•çš„å…³é”®è¯æœç´¢",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯ï¼ˆç®€å•æœç´¢ï¼‰æˆ–å®Œæ•´çš„ Elasticsearch DSL æŸ¥è¯¢ JSON å­—ç¬¦ä¸²"
                    },
                    "time_range": {
                        "type": "string",
                        "description": "æ—¶é—´èŒƒå›´ï¼Œå¦‚ 'now-1h', 'now-24h', 'now-7d' ç­‰",
                        "default": "now-1h"
                    },
                    "size": {
                        "type": "integer",
                        "description": "è¿”å›ç»“æœæ•°é‡",
                        "default": 20,
                        "minimum": 1,
                        "maximum": 1000
                    },
                    "index_pattern": {
                        "type": "string",
                        "description": "ç´¢å¼•æ¨¡å¼",
                        "default": "logstash-*"
                    },
                    "fields": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "è¦è¿”å›çš„å­—æ®µåˆ—è¡¨",
                        "default": ["@timestamp", "kubernetes.container_name", "log"]
                    }
                },
                "required": ["query"]
            }
        ),
        Tool(
            name="kibana_aggregate_logs",
            description="èšåˆç»Ÿè®¡ Kibana æ—¥å¿—ã€‚å¯ä»¥æŒ‰æœåŠ¡ã€æ—¶é—´ç­‰ç»´åº¦ç»Ÿè®¡",
            inputSchema={
                "type": "object",
                "properties": {
                    "aggregation_type": {
                        "type": "string",
                        "enum": ["by_service", "by_time", "custom"],
                        "description": "èšåˆç±»å‹: by_service(æŒ‰æœåŠ¡ç»Ÿè®¡), by_time(æŒ‰æ—¶é—´ç»Ÿè®¡), custom(è‡ªå®šä¹‰èšåˆ)"
                    },
                    "time_range": {
                        "type": "string",
                        "description": "æ—¶é—´èŒƒå›´ï¼Œå¦‚ 'now-1h', 'now-24h' ç­‰",
                        "default": "now-1h"
                    },
                    "filter": {
                        "type": "string",
                        "description": "å¯é€‰çš„è¿‡æ»¤æ¡ä»¶ï¼ˆElasticsearch DSL JSON å­—ç¬¦ä¸²ï¼‰"
                    },
                    "custom_aggregation": {
                        "type": "string",
                        "description": "è‡ªå®šä¹‰èšåˆæŸ¥è¯¢ï¼ˆä»…å½“ aggregation_type ä¸º 'custom' æ—¶ä½¿ç”¨ï¼‰"
                    },
                    "index_pattern": {
                        "type": "string",
                        "description": "ç´¢å¼•æ¨¡å¼",
                        "default": "logstash-*"
                    }
                },
                "required": ["aggregation_type"]
            }
        ),
        Tool(
            name="kibana_get_latest_logs",
            description="å¿«é€Ÿè·å–æœ€æ–°çš„æ—¥å¿—è®°å½•",
            inputSchema={
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "æœåŠ¡åç§°ï¼ˆkubernetes.container_nameï¼‰ï¼Œä¸ºç©ºåˆ™è¿”å›æ‰€æœ‰æœåŠ¡"
                    },
                    "size": {
                        "type": "integer",
                        "description": "è¿”å›æ•°é‡",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 100
                    },
                    "index_pattern": {
                        "type": "string",
                        "description": "ç´¢å¼•æ¨¡å¼",
                        "default": "logstash-*"
                    }
                }
            }
        ),
        Tool(
            name="kibana_search_errors",
            description="æœç´¢é”™è¯¯æ—¥å¿—ï¼ˆåŒ…å« errorã€exceptionã€fail ç­‰å…³é”®è¯ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "service": {
                        "type": "string",
                        "description": "æœåŠ¡åç§°ï¼ˆkubernetes.container_nameï¼‰ï¼Œä¸ºç©ºåˆ™æœç´¢æ‰€æœ‰æœåŠ¡"
                    },
                    "severity": {
                        "type": "string",
                        "enum": ["error", "exception", "critical", "all"],
                        "description": "é”™è¯¯ä¸¥é‡ç¨‹åº¦",
                        "default": "all"
                    },
                    "time_range": {
                        "type": "string",
                        "description": "æ—¶é—´èŒƒå›´",
                        "default": "now-1h"
                    },
                    "size": {
                        "type": "integer",
                        "description": "è¿”å›æ•°é‡",
                        "default": 20,
                        "minimum": 1,
                        "maximum": 100
                    },
                    "index_pattern": {
                        "type": "string",
                        "description": "ç´¢å¼•æ¨¡å¼",
                        "default": "logstash-*"
                    }
                }
            }
        ),
        Tool(
            name="kibana_raw_query",
            description="æ‰§è¡ŒåŸå§‹çš„ Elasticsearch DSL æŸ¥è¯¢ï¼ˆé«˜çº§ç”¨æˆ·ï¼‰",
            inputSchema={
                "type": "object",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Elasticsearch è·¯å¾„ï¼Œå¦‚ '/logstash-*/_search'",
                        "default": "/logstash-*/_search"
                    },
                    "query": {
                        "type": "string",
                        "description": "å®Œæ•´çš„ Elasticsearch DSL æŸ¥è¯¢ JSON å­—ç¬¦ä¸²"
                    }
                },
                "required": ["query"]
            }
        )
    ]


@app.call_tool()
async def call_tool(name: str, arguments: Any) -> list[TextContent]:
    """å¤„ç†å·¥å…·è°ƒç”¨"""
    
    if name == "kibana_set_credentials":
        username = arguments["username"]
        password = arguments["password"]
        set_credentials(username, password)
        return [TextContent(
            type="text",
            text=f"âœ… Kibana å‡­è¯å·²è®¾ç½®\nç”¨æˆ·å: {username}\nä¼šè¯å°†åœ¨é¦–æ¬¡è¯·æ±‚æ—¶è‡ªåŠ¨å»ºç«‹"
        )]
    
    # å…¶ä»–å·¥å…·éƒ½éœ€è¦å…ˆéªŒè¯ä¼šè¯
    session = get_session()
    
    if name == "kibana_search_logs":
        return await handle_search_logs(session, arguments)
    
    elif name == "kibana_aggregate_logs":
        return await handle_aggregate_logs(session, arguments)
    
    elif name == "kibana_get_latest_logs":
        return await handle_get_latest_logs(session, arguments)
    
    elif name == "kibana_search_errors":
        return await handle_search_errors(session, arguments)
    
    elif name == "kibana_raw_query":
        return await handle_raw_query(session, arguments)
    
    else:
        raise ValueError(f"æœªçŸ¥å·¥å…·: {name}")


async def handle_search_logs(session: KibanaSession, args: dict) -> list[TextContent]:
    """å¤„ç†æ—¥å¿—æœç´¢"""
    query_str = args["query"]
    time_range = args.get("time_range", "now-1h")
    size = args.get("size", 20)
    index_pattern = args.get("index_pattern", "logstash-*")
    fields = args.get("fields", ["@timestamp", "kubernetes.container_name", "log"])
    
    # å°è¯•è§£æä¸º JSONï¼ˆå®Œæ•´çš„ DSL æŸ¥è¯¢ï¼‰
    try:
        query_dsl = json.loads(query_str)
    except json.JSONDecodeError:
        # ç®€å•å…³é”®è¯æœç´¢
        query_dsl = {
            "query": {
                "bool": {
                    "must": [
                        {"match": {"log": query_str}}
                    ],
                    "filter": [
                        {"range": {"@timestamp": {"gte": time_range}}}
                    ]
                }
            },
            "size": size,
            "_source": fields,
            "sort": [{"@timestamp": {"order": "desc"}}]
        }
    
    # ç¡®ä¿æœ‰ size å’Œ _source
    if "size" not in query_dsl:
        query_dsl["size"] = size
    if "_source" not in query_dsl:
        query_dsl["_source"] = fields
    
    path = f"/{index_pattern}/_search"
    result = await session.request(path, query_dsl)
    
    # æ ¼å¼åŒ–è¾“å‡º
    hits = result.get("hits", {}).get("hits", [])
    total = result.get("hits", {}).get("total", {}).get("value", 0)
    
    output = f"ğŸ“Š æ‰¾åˆ° {total} æ¡æ—¥å¿—ï¼Œæ˜¾ç¤ºå‰ {len(hits)} æ¡ï¼š\n\n"
    
    for i, hit in enumerate(hits, 1):
        source = hit.get("_source", {})
        timestamp = source.get("@timestamp", "N/A")
        service = source.get("kubernetes", {}).get("container_name", "N/A")
        log = source.get("log", "N/A")
        
        output += f"{i}. [{timestamp}] {service}\n"
        output += f"   {log[:200]}{'...' if len(log) > 200 else ''}\n\n"
    
    return [TextContent(type="text", text=output)]


async def handle_aggregate_logs(session: KibanaSession, args: dict) -> list[TextContent]:
    """å¤„ç†æ—¥å¿—èšåˆ"""
    agg_type = args["aggregation_type"]
    time_range = args.get("time_range", "now-1h")
    index_pattern = args.get("index_pattern", "logstash-*")
    
    # åŸºç¡€æŸ¥è¯¢
    base_query = {
        "range": {"@timestamp": {"gte": time_range}}
    }
    
    # æ·»åŠ è¿‡æ»¤æ¡ä»¶
    filter_query = args.get("filter")
    if filter_query:
        try:
            filter_dsl = json.loads(filter_query)
            base_query = {"bool": {"must": [base_query, filter_dsl]}}
        except json.JSONDecodeError:
            pass
    
    if agg_type == "by_service":
        query_dsl = {
            "size": 0,
            "query": base_query,
            "aggs": {
                "services": {
                    "terms": {
                        "field": "kubernetes.container_name.keyword",
                        "size": 20,
                        "order": {"_count": "desc"}
                    }
                }
            }
        }
    elif agg_type == "by_time":
        query_dsl = {
            "size": 0,
            "query": base_query,
            "aggs": {
                "logs_over_time": {
                    "date_histogram": {
                        "field": "@timestamp",
                        "fixed_interval": "5m"
                    }
                }
            }
        }
    elif agg_type == "custom":
        custom_agg = args.get("custom_aggregation")
        if not custom_agg:
            raise ValueError("custom èšåˆç±»å‹éœ€è¦æä¾› custom_aggregation å‚æ•°")
        try:
            agg_dsl = json.loads(custom_agg)
            query_dsl = {
                "size": 0,
                "query": base_query,
                "aggs": agg_dsl
            }
        except json.JSONDecodeError as e:
            raise ValueError(f"custom_aggregation ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {str(e)}")
    else:
        raise ValueError(f"æœªçŸ¥çš„èšåˆç±»å‹: {agg_type}")
    
    path = f"/{index_pattern}/_search"
    result = await session.request(path, query_dsl)
    
    # æ ¼å¼åŒ–è¾“å‡º
    aggs = result.get("aggregations", {})
    
    if agg_type == "by_service":
        buckets = aggs.get("services", {}).get("buckets", [])
        output = f"ğŸ“ˆ æœåŠ¡æ—¥å¿—ç»Ÿè®¡ï¼ˆæ—¶é—´èŒƒå›´: {time_range}ï¼‰:\n\n"
        for bucket in buckets:
            service = bucket["key"]
            count = bucket["doc_count"]
            output += f"  â€¢ {service}: {count:,} æ¡\n"
    elif agg_type == "by_time":
        buckets = aggs.get("logs_over_time", {}).get("buckets", [])
        output = f"ğŸ“ˆ æ—¶é—´åºåˆ—æ—¥å¿—ç»Ÿè®¡ï¼ˆæ—¶é—´èŒƒå›´: {time_range}ï¼‰:\n\n"
        for bucket in buckets:
            timestamp = bucket["key_as_string"]
            count = bucket["doc_count"]
            output += f"  â€¢ {timestamp}: {count:,} æ¡\n"
    else:
        output = f"ğŸ“ˆ è‡ªå®šä¹‰èšåˆç»“æœ:\n\n{json.dumps(aggs, indent=2, ensure_ascii=False)}"
    
    return [TextContent(type="text", text=output)]


async def handle_get_latest_logs(session: KibanaSession, args: dict) -> list[TextContent]:
    """è·å–æœ€æ–°æ—¥å¿—"""
    service = args.get("service")
    size = args.get("size", 10)
    index_pattern = args.get("index_pattern", "logstash-*")
    
    query_dsl = {
        "query": {"match_all": {}},
        "size": size,
        "sort": [{"@timestamp": {"order": "desc"}}],
        "_source": ["@timestamp", "kubernetes.container_name", "log"]
    }
    
    # æ·»åŠ æœåŠ¡è¿‡æ»¤
    if service:
        query_dsl["query"] = {
            "match": {
                "kubernetes.container_name": service
            }
        }
    
    path = f"/{index_pattern}/_search"
    result = await session.request(path, query_dsl)
    
    hits = result.get("hits", {}).get("hits", [])
    
    output = f"ğŸ“ æœ€æ–° {len(hits)} æ¡æ—¥å¿—"
    if service:
        output += f"ï¼ˆæœåŠ¡: {service}ï¼‰"
    output += ":\n\n"
    
    for i, hit in enumerate(hits, 1):
        source = hit.get("_source", {})
        timestamp = source.get("@timestamp", "N/A")
        svc = source.get("kubernetes", {}).get("container_name", "N/A")
        log = source.get("log", "N/A")
        
        output += f"{i}. [{timestamp}] {svc}\n"
        output += f"   {log[:200]}{'...' if len(log) > 200 else ''}\n\n"
    
    return [TextContent(type="text", text=output)]


async def handle_search_errors(session: KibanaSession, args: dict) -> list[TextContent]:
    """æœç´¢é”™è¯¯æ—¥å¿—"""
    service = args.get("service")
    severity = args.get("severity", "all")
    time_range = args.get("time_range", "now-1h")
    size = args.get("size", 20)
    index_pattern = args.get("index_pattern", "logstash-*")
    
    # æ„å»ºé”™è¯¯å…³é”®è¯
    error_keywords = {
        "error": ["error", "Error", "ERROR"],
        "exception": ["exception", "Exception", "EXCEPTION"],
        "critical": ["critical", "Critical", "CRITICAL", "fatal", "Fatal", "FATAL"],
        "all": ["error", "Error", "ERROR", "exception", "Exception", "fail", "Fail", "FAIL"]
    }
    
    keywords = error_keywords.get(severity, error_keywords["all"])
    
    # æ„å»ºæŸ¥è¯¢
    must_clauses = [
        {
            "bool": {
                "should": [
                    {"match": {"log": keyword}} for keyword in keywords
                ],
                "minimum_should_match": 1
            }
        }
    ]
    
    if service:
        must_clauses.append({
            "match": {"kubernetes.container_name": service}
        })
    
    query_dsl = {
        "query": {
            "bool": {
                "must": must_clauses,
                "filter": [
                    {"range": {"@timestamp": {"gte": time_range}}}
                ]
            }
        },
        "size": size,
        "sort": [{"@timestamp": {"order": "desc"}}],
        "_source": ["@timestamp", "kubernetes.container_name", "log"]
    }
    
    path = f"/{index_pattern}/_search"
    result = await session.request(path, query_dsl)
    
    hits = result.get("hits", {}).get("hits", [])
    total = result.get("hits", {}).get("total", {}).get("value", 0)
    
    output = f"ğŸš¨ æ‰¾åˆ° {total} æ¡é”™è¯¯æ—¥å¿—"
    if service:
        output += f"ï¼ˆæœåŠ¡: {service}ï¼‰"
    output += f"ï¼Œæ˜¾ç¤ºå‰ {len(hits)} æ¡:\n\n"
    
    for i, hit in enumerate(hits, 1):
        source = hit.get("_source", {})
        timestamp = source.get("@timestamp", "N/A")
        svc = source.get("kubernetes", {}).get("container_name", "N/A")
        log = source.get("log", "N/A")
        
        output += f"{i}. [{timestamp}] {svc}\n"
        output += f"   {log[:300]}{'...' if len(log) > 300 else ''}\n\n"
    
    return [TextContent(type="text", text=output)]


async def handle_raw_query(session: KibanaSession, args: dict) -> list[TextContent]:
    """æ‰§è¡ŒåŸå§‹æŸ¥è¯¢"""
    path = args.get("path", "/logstash-*/_search")
    query_str = args["query"]
    
    try:
        query_dsl = json.loads(query_str)
    except json.JSONDecodeError as e:
        raise ValueError(f"æŸ¥è¯¢ä¸æ˜¯æœ‰æ•ˆçš„ JSON: {str(e)}")
    
    result = await session.request(path, query_dsl)
    
    # æ ¼å¼åŒ– JSON è¾“å‡º
    output = json.dumps(result, indent=2, ensure_ascii=False)
    
    return [TextContent(type="text", text=output)]


async def main():
    """å¯åŠ¨ MCP server"""
    from mcp.server.stdio import stdio_server
    
    async with stdio_server() as (read_stream, write_stream):
        await app.run(
            read_stream,
            write_stream,
            app.create_initialization_options()
        )

def run():
    """åŒæ­¥å…¥å£ç‚¹ï¼Œç”¨äº console_scripts"""
    asyncio.run(main())

if __name__ == "__main__":
    run()
